---
title: "크롤러 봇이 서버를 죽이고 있었다"
description: "크롤링 봇이 슬로우쿼리 API를 반복 호출하던 원인을 추적하고 백엔드 부하 50% 이상을 줄인 과정"
date: "2025-07-15"
category: "트러블슈팅"
tags: ["트러블슈팅", "SEO", "성능", "모니터링"]
---

## 들어가며

와탭에서 알림이 울렸다.

API request point가 HIGH로 찍히고 있었다. 간헐적으로 튀는 수준이 아니라, 지속적으로 높은 상태가 유지되고 있었다. 히트맵을 열어보니 점들이 응답시간 5초 라인까지 빽빽하게 쏟아지고 있었다.

서비스에 장애가 난 건 아니었다. 사용자 문의도 없었다. 하지만 이대로 두면 언제 터져도 이상하지 않은 상태였다. 원인을 찾아야 했다.

---

## 히트맵에 점이 쏟아진다

![와탭 히트맵 — 트랜잭션 점들이 응답시간 5초까지 빽빽하게 찍혀있다](/assets/images/heatmap.webp)

와탭 히트맵은 API 트랜잭션의 응답 시간을 시각적으로 보여준다. 점 하나가 API 요청 하나이고, 위로 올라갈수록 응답이 느린 것이다. 정상이면 아래쪽에 점이 드문드문 찍혀야 하는데, 화면을 보니 점들이 5초 라인까지 빽빽하게 차올라 있었다. 사이사이에 주황색 경고 점들도 보였다.

특정 API 엔드포인트에 비정상적으로 많은 요청이 몰리고 있었다. 처음에는 사용자 트래픽이 급증한 건가 싶었다. 마케팅 푸시라도 나갔나 확인해봤지만, 아무것도 없었다.

패턴을 좀 더 들여다보니 이상한 점이 보였다. 요청이 특정 동적 라우팅 페이지에만 집중되어 있었고, 요청 간격이 기계적으로 일정했다. 사람의 행동 패턴이 아니었다.

---

## 범인은 크롤러 봇이었다

서버 로그를 까봤다. user-agent를 확인하니 답이 나왔다.

```
Googlebot/2.1 (+http://www.google.com/bot.html)
Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)
Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)
Mozilla/5.0 (compatible; AhrefsBot/7.0; +http://ahrefs.com/robot/)
```

구글봇, 빙봇, 얀덱스봇, Ahrefs봇. 여러 크롤러가 동시에 사이트를 긁고 있었다.

문제의 흐름은 이랬다.

```
1. 크롤러가 sitemap.xml을 읽는다
   ↓
2. 사이트맵에 노출된 수천 개의 동적 라우팅 URL을 수집한다
   ↓
3. 각 URL에 접근한다 (서버사이드 렌더링 트리거)
   ↓
4. 서버 컴포넌트에서 백엔드 API를 호출한다
   ↓
5. 이 중 일부가 슬로우쿼리를 실행하는 API다
   ↓
6. 여러 크롤러가 동시에 → 슬로우쿼리가 병렬 실행 → DB 부하 폭증
```

사이트맵에는 정책 상세, 지원사업 상세 같은 동적 페이지 URL이 수천 개 있었다. 크롤러 입장에서는 사이트맵에 있으니까 당연히 전부 긁는 거다. 그게 크롤러의 일이니까.

문제는 이 페이지들의 서버 컴포넌트가 단순히 본문 데이터만 가져오는 게 아니었다는 점이다. 관련 추천 데이터, 통계 데이터 등 여러 API를 호출하고 있었고, 그중 일부가 DB에 부하를 주는 슬로우쿼리였다.

크롤러 한 대만 있었으면 괜찮았을 수도 있다. 하지만 구글, 빙, 얀덱스, Ahrefs가 각자 독립적으로 같은 URL 수천 개를 동시에 크롤링하고 있었다. 사용자는 1명도 불편을 겪지 않았는데, 봇이 서버를 죽이고 있었다.

---

## 해결: user-agent 기반 조건부 처리

크롤러를 차단하는 건 답이 아니었다. `robots.txt`로 막으면 SEO에 직접적인 악영향이 생긴다. 크롤링은 허용하되, 크롤러의 요청을 가볍게 만들어야 했다.

3단계로 접근했다.

### 1단계: user-agent 리스트업

먼저 서버에 접근하는 주요 크롤러 봇을 식별했다.

```typescript
const BOT_USER_AGENTS = [
  'Googlebot',
  'Bingbot',
  'Yandex',
  'AhrefsBot',
  'SemrushBot',
  'DotBot',
  'PetalBot',
];

const isBot = (userAgent: string) =>
  BOT_USER_AGENTS.some((bot) => userAgent.includes(bot));
```

### 2단계: 서버사이드 user-agent 포워딩

기존에는 서버 컴포넌트에서 백엔드 API를 호출할 때 user-agent를 전달하지 않았다. 그래서 백엔드 로그에서는 모든 요청이 Next.js 서버에서 온 것으로만 보였고, 크롤러가 원인이라는 걸 파악하기 어려웠다.

서버사이드 API 요청 시 클라이언트의 user-agent를 그대로 포워딩하도록 변경했다.

```typescript
import { headers } from 'next/headers';

const requestHeaders = await headers();
const userAgent = requestHeaders.get('user-agent') ?? '';

const response = await fetch(`${API_BASE}/policy/${id}`, {
  headers: {
    'User-Agent': userAgent, // 원래 요청자의 user-agent 포워딩
  },
});
```

이제 백엔드 로그에서 "이 API를 누가 호출했는지"가 보이기 시작했다. 어떤 봇이, 어떤 API를, 얼마나 호출하는지 정확하게 측정할 수 있게 되었다.

### 3단계: 봇일 때 무거운 API 호출 스킵

핵심은 **SEO에 필요한 데이터와 불필요한 데이터를 분리**하는 것이었다.

페이지의 데이터를 두 종류로 나눴다.

- **SEO 필수 데이터**: 정책 제목, 본문, 메타데이터 — 크롤러에게 반드시 제공해야 함
- **부가 데이터**: 관련 정책 추천, 조회 통계, 유사 지원사업 — 사용자 경험을 위한 것이지 크롤러에게는 불필요

```typescript
import { headers } from 'next/headers';

const requestHeaders = await headers();
const userAgent = requestHeaders.get('user-agent') ?? '';
const botRequest = isBot(userAgent);

// SEO 필수 데이터 — 봇에게도 제공
const policyDetail = await fetchPolicyDetail(id);

// 무거운 부가 데이터 — 봇이면 스킵
const relatedPolicies = botRequest ? null : await fetchRelatedPolicies(id);
const viewStats = botRequest ? null : await fetchViewStatistics(id);
```

크롤러가 접근하면 본문과 메타데이터만 빠르게 서빙하고, 슬로우쿼리가 필요한 추천/통계 API는 호출하지 않는다. 사용자가 접근하면 기존과 동일하게 모든 데이터를 보여준다.

---

## 결과

적용 후 와탭 히트맵의 트랜잭션 밀도가 눈에 띄게 줄었다.

백엔드 부하가 **와탭 기준 50% 이상 감축**되었다. 크롤러는 여전히 같은 URL에 접근하지만, 서버가 처리하는 작업량이 절반 이하로 줄어든 것이다.

그리고 SEO에는 영향이 없었다. 크롤러에게 핵심 콘텐츠(제목, 본문, 메타데이터)는 그대로 서빙하고 있기 때문이다. Google Search Console에서 인덱싱 상태를 확인했을 때도 정상이었다.

정리하면 이렇다.

```
항목            개선 전                      개선 후
─────────────────────────────────────────────────────────────
백엔드 부하     HIGH 지속 (히트맵 과밀)          50% 이상 감축 (정상 범위)
크롤러 대응     모든 API 동일하게 호출          봇은 필수 데이터만 서빙
SEO 영향       -                            영향 없음 (인덱싱 정상)
사용자 경험     변화 없음                      변화 없음
```

---

## 돌아보며

이번 장애 대응에서 배운 것이 몇 가지 있다.

첫째, **서버 부하 = 사용자 트래픽이라는 가정은 틀릴 수 있다.** 실제 사용자는 아무 문제 없이 서비스를 이용하고 있었는데, 백엔드는 비명을 지르고 있었다. 사용자가 아닌 봇이 부하의 주범이라는 건, 로그를 까보기 전까지는 생각하기 어려운 방향이다.

둘째, **모니터링 도구가 없었으면 원인 파악이 훨씬 늦었을 것이다.** 와탭 히트맵에서 트랜잭션이 비정상적으로 쌓이는 걸 실시간으로 볼 수 있었기 때문에 빠르게 대응할 수 있었다. 만약 "사용자 문의가 올 때까지 모르고 있었다"면 상황이 훨씬 심각해졌을 것이다.

셋째, **크롤러를 막는 게 아니라 요청을 가볍게 만드는 것이 핵심이다.** `robots.txt`로 차단하면 간단하지만, SEO를 포기하는 셈이다. 크롤링은 허용하되 불필요한 서버 부하만 줄이는 방향이 맞았다. 봇에게 필요한 건 콘텐츠이지, 추천 알고리즘의 결과가 아니니까.
